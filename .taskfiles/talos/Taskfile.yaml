---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: '3'

tasks:

  apply-node:
    desc: Apply Talos config to a node [CLUSTER=main] [HOSTNAME=required]
    dotenv: ['{{.CLUSTER_DIR}}/cluster.env']
    cmds:
      - task: down
      - sops exec-file --input-type yaml --output-type yaml {{.CLUSTER_DIR}}/talos/{{.HOSTNAME}}.sops.yaml.j2 "minijinja-cli {}" | talosctl --nodes {{.HOSTNAME}} apply-config --mode=staged --file /dev/stdin
      - talosctl --nodes {{.HOSTNAME}} reboot
      - talosctl --nodes {{.HOSTNAME}} health --wait-timeout=10m --server=false
      - task: up
    requires:
      vars: [CLUSTER, HOSTNAME]
    preconditions:
      - test -f {{.CLUSTER_DIR}}/cluster.env
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - test -f {{.CLUSTER_DIR}}/talos/{{.HOSTNAME}}.sops.yaml.j2
      - talosctl --nodes {{.HOSTNAME}} get machineconfig &>/dev/null

  upgrade-node:
    desc: Upgrade Talos on a single node [CLUSTER=main] [HOSTNAME=required]
    dotenv: ['{{.CLUSTER_DIR}}/cluster.env']
    cmds:
      - task: down
      - talosctl --nodes {{.HOSTNAME}} upgrade --image="factory.talos.dev/installer/{{.SCHEMATIC_ID}}:$TALOS_VERSION" --timeout=10m
      - talosctl --nodes {{.HOSTNAME}} health --wait-timeout=10m --server=false
      - task: up
    vars:
      SCHEMATIC_ID:
        sh: kubectl get node {{.HOSTNAME}} --output=jsonpath='{.metadata.annotations.extensions\.talos\.dev/schematic}'
    requires:
      vars: [CLUSTER, HOSTNAME]
    preconditions:
      - test -f {{.CLUSTER_DIR}}/cluster.env
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - talosctl config info &>/dev/null
      - talosctl --nodes {{.HOSTNAME}} get machineconfig &>/dev/null
      - curl -fsSL -o /dev/null --fail https://github.com/siderolabs/talos/releases/tag/$TALOS_VERSION

  upgrade-k8s:
    desc: Upgrade Kubernetes across the whole cluster [CLUSTER=main] [VERSION=required]
    dotenv: ['{{.CLUSTER_DIR}}/cluster.env']
    cmds:
      - task: down
      - talosctl --nodes {{.CONTROLLER}} upgrade-k8s --to $KUBERNETES_VERSION
      - task: up
    vars:
      CONTROLLER:
        sh: talosctl config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    requires:
      vars: [CLUSTER]
    preconditions:
      - test -f {{.CLUSTER_DIR}}/cluster.env
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - talosctl config info &>/dev/null
      - talosctl --nodes {{.CONTROLLER}} get machineconfig &>/dev/null
      - curl -fsSL -o /dev/null --fail https://github.com/siderolabs/kubelet/releases/tag/$KUBERNETES_VERSION

  reboot-node:
    desc: Reboot Talos on a single node [CLUSTER=main] [HOSTNAME=required]
    cmds:
      - task: down
      - talosctl --nodes {{.HOSTNAME}} reboot
      - talosctl --nodes {{.HOSTNAME}} health --wait-timeout=10m --server=false
      - task: up
    requires:
      vars: [CLUSTER, HOSTNAME]
    preconditions:
      - test -f {{.CLUSTER_DIR}}/cluster.env
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - talosctl config info &>/dev/null
      - talosctl --nodes {{.HOSTNAME}} get machineconfig &>/dev/null

  reset-node:
    desc: Reset Talos on a single node [CLUSTER=main] [HOSTNAME=required]
    prompt: Reset Talos node '{{.HOSTNAME}}' on the '{{.CLUSTER}}' cluster ... continue?
    cmd: talosctl reset --nodes {{.HOSTNAME}} --graceful=false
    requires:
      vars: [CLUSTER, HOSTNAME]
    preconditions:
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - talosctl config info &>/dev/null
      - talosctl --nodes {{.HOSTNAME}} get machineconfig &>/dev/null

  reset-cluster:
    desc: Reset Talos across the whole cluster [CLUSTER=main]
    prompt: Reset Talos cluster '{{.CLUSTER}}' ... continue?
    cmd: talosctl reset --nodes {{.HOSTNAMES}} --graceful=false
    vars:
      HOSTNAMES:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(",")'
    requires:
      vars: [CLUSTER]
    preconditions:
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - talosctl config info &>/dev/null
      - talosctl --nodes {{.NODES}} get machineconfig &>/dev/null

  down:
    internal: true
    cmds:
      - flux --namespace flux-system suspend kustomization --all
      - until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
      - task: :volsync:state-suspend

  up:
    internal: true
    cmds:
      - flux --namespace flux-system resume kustomization --all
      - until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
      - task: :volsync:state-resume
