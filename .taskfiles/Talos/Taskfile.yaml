---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

x-env-vars: &env-vars
  TALOS_VERSION:
    sh: yq 'select(document_index == 1).spec.postBuild.substitute.TALOS_VERSION' {{.KUBERNETES_DIR}}/{{.cluster}}/apps/system-upgrade/system-upgrade-controller/ks.yaml
  TALOS_SCHEMATIC_ID:
    sh: yq 'select(document_index == 1).spec.postBuild.substitute.TALOS_SCHEMATIC_ID' {{.KUBERNETES_DIR}}/{{.cluster}}/apps/system-upgrade/system-upgrade-controller/ks.yaml
  KUBERNETES_VERSION:
    sh: yq 'select(document_index == 1).spec.postBuild.substitute.KUBERNETES_VERSION' {{.KUBERNETES_DIR}}/{{.cluster}}/apps/system-upgrade/system-upgrade-controller/ks.yaml

vars:
  # Ref: https://github.com/onedr0p/home-service
  HOME_SERVICE_ADDR: voyager.internal
  HOME_SERVICE_USER: devin
  HOME_SERVICE_MATCHBOX_DIR: /var/opt/home-service/apps/matchbox/data/config

tasks:

  bootstrap:
    desc: Bootstrap Talos
    prompt: Bootstrap Talos on the '{{.cluster}}' cluster ... continue?
    cmds:
      - task: bootstrap-etcd
        vars: { cluster: "{{.cluster}}" }
      - task: fetch-kubeconfig
        vars: { cluster: "{{.cluster}}" }
      - task: bootstrap-apps
        vars: { cluster: "{{.cluster}}" }
    requires:
      vars: ["cluster"]

  bootstrap-etcd:
    desc: Bootstrap Etcd
    cmd: until talosctl --context {{.cluster}} --nodes {{.controller}} bootstrap; do sleep 10; done
    vars:
      controller:
        sh: talosctl --context {{.cluster}} config info --output json | jq --raw-output '.endpoints[0]'
    requires:
      vars: ["cluster"]
    preconditions:
      - test -f {{.KUBERNETES_DIR}}/{{.cluster}}/talosconfig
      - talosctl --context {{.cluster}} config info >/dev/null 2>&1

  bootstrap-apps:
    desc: Bootstrap core apps needed for Talos
    cmds:
      - until kubectl --context {{.cluster}} wait --for=condition=Ready=False nodes --all --timeout=10m; do sleep 10; done
      - helmfile --quiet --kube-context {{.cluster}} --file {{.KUBERNETES_DIR}}/{{.cluster}}/bootstrap/helmfile.yaml apply --skip-diff-on-install --suppress-diff
      - until kubectl --context {{.cluster}} wait --for=condition=Ready nodes --all --timeout=10m; do sleep 10; done
    requires:
      vars: ["cluster"]
    preconditions:
      - test -f {{.KUBERNETES_DIR}}/{{.cluster}}/talosconfig
      - talosctl --context {{.cluster}} config info >/dev/null 2>&1
      - test -f {{.KUBERNETES_DIR}}/{{.cluster}}/bootstrap/helmfile.yaml

  fetch-kubeconfig:
    desc: Fetch kubeconfig from Talos controllers
    cmd: |
      talosctl --context {{.cluster}} kubeconfig --nodes {{.controller}} \
          --force --force-context-name {{.cluster}} {{.KUBERNETES_DIR}}/{{.cluster}}
    vars:
      controller:
        sh: talosctl --context {{.cluster}} config info --output json | jq --raw-output '.endpoints[0]'
    requires:
      vars: ["cluster"]
    preconditions:
      - test -f {{.KUBERNETES_DIR}}/{{.cluster}}/talosconfig
      - talosctl --context {{.cluster}} config info >/dev/null 2>&1

  apply-config:
    desc: Apply Talos configuration to a node
    cmd: |
      sops -d {{.KUBERNETES_DIR}}/{{.cluster}}/bootstrap/talos/assets/{{.hostname}}.secret.sops.yaml | \
          envsubst | \
              talosctl --context {{.cluster}} apply-config --mode={{.mode}} --nodes {{.node}} --file /dev/stdin
    env: *env-vars
    vars:
      mode: '{{.mode | default "no-reboot"}}'
      hostname:
        sh: talosctl --context {{.cluster}} --nodes {{.node}} get machineconfig -o jsonpath='{.spec.machine.network.hostname}'
    requires:
      vars: ["cluster", "node"]
    preconditions:
      - test -f {{.KUBERNETES_DIR}}/{{.cluster}}/talosconfig
      - test -f {{.KUBERNETES_DIR}}/{{.cluster}}/bootstrap/talos/assets/{{.hostname}}.secret.sops.yaml
      - talosctl --context {{.cluster}} --nodes {{.node}} get machineconfig >/dev/null 2>&1

  upgrade:
    desc: Upgrade Talos on a node
    cmds:
      - until kubectl --context {{.cluster}} wait --timeout=5m --for=condition=Complete jobs --all --all-namespaces; do sleep 10; done
      - talosctl --context {{.cluster}} --nodes {{.node}} upgrade --image="factory.talos.dev/installer/{{.TALOS_SCHEMATIC_ID}}:{{.TALOS_VERSION}}" --wait=true --timeout=10m --preserve=true
      - talosctl --context {{.cluster}} --nodes {{.node}} health --wait-timeout=10m --server=false
      - until kubectl --context {{.cluster}} wait --timeout=5m --for=jsonpath=.status.ceph.health=HEALTH_OK cephcluster --all --all-namespaces; do sleep 10; done
    vars: *env-vars
    requires:
      vars: ["cluster", "node"]
    preconditions:
      - test -f {{.KUBERNETES_DIR}}/{{.cluster}}/talosconfig
      - talosctl --context {{.cluster}} config info >/dev/null 2>&1
      - talosctl --context {{.cluster}} --nodes {{.node}} get machineconfig >/dev/null 2>&1

  upgrade-k8s:
    desc: Upgrade Kubernetes on a controller node
    cmds:
      - until kubectl --context {{.cluster}} wait --timeout=5m --for=condition=Complete jobs --all --all-namespaces; do sleep 10; done
      - talosctl --context {{.cluster}} --nodes {{.controller}} upgrade-k8s --to {{.KUBERNETES_VERSION}}
    vars: *env-vars
    requires:
      vars: ["cluster", "controller"]
    preconditions:
      - test -f {{.KUBERNETES_DIR}}/{{.cluster}}/talosconfig
      - talosctl --context {{.cluster}} config info >/dev/null 2>&1
      - talosctl --context {{.cluster}} --nodes {{.node}} get machineconfig >/dev/null 2>&1

  reset-node:
    desc: Reset a Talos node and shut it down
    prompt: Reset Talos '{{.node}}' node on the '{{.cluster}}' cluster ... continue?
    cmd: talosctl --context {{.cluster}} reset --nodes {{.node}} --graceful=false
    requires:
      vars: ["cluster", "node"]
    preconditions:
      - test -f {{.KUBERNETES_DIR}}/{{.cluster}}/talosconfig
      - talosctl --context {{.cluster}} config info >/dev/null 2>&1
      - talosctl --context {{.cluster}} --nodes {{.node}} get machineconfig >/dev/null 2>&1

  reset-cluster:
    desc: Reset all the Talos nodes and shut 'em down
    prompt: Reset Talos on the '{{.cluster}}' cluster ... continue?
    cmd: talosctl --context {{.cluster}} reset --nodes {{.nodes}} --graceful=false
    vars:
      nodes:
        sh: talosctl --context {{.cluster}} config info --output json | jq --join-output '[.nodes[]] | join(",")'
    requires:
      vars: ["cluster"]
    preconditions:
      - test -f {{.KUBERNETES_DIR}}/{{.cluster}}/talosconfig
      - talosctl --context {{.cluster}} config info >/dev/null 2>&1
      - talosctl --context {{.cluster}} --nodes {{.nodes}} get machineconfig >/dev/null 2>&1

  bootstrap-matchbox:
    desc: Bootstrap required Matchbox configuration to PXE Boot machine
    cmds:
      - for: ["kernel-amd64", "initramfs-amd64.xz"]
        cmd: |
          curl -skL https://factory.talos.dev/image/{{.TALOS_SCHEMATIC_ID}}/{{.TALOS_VERSION}}/{{.ITEM}} | \
              curl -skT - -u "{{.HOME_SERVICE_USER}}:" \
                  sftp://{{.HOME_SERVICE_ADDR}}/{{.HOME_SERVICE_MATCHBOX_DIR}}/assets/{{.ITEM}}
      - find {{.KUBERNETES_DIR}}/{{.cluster}}/bootstrap/talos/assets -type f | xargs -I{} sh -c "sops -d {} | envsubst | curl -skT - -u "{{.HOME_SERVICE_USER}}:" sftp://{{.HOME_SERVICE_ADDR}}/{{.HOME_SERVICE_MATCHBOX_DIR}}/assets/\$(basename {} | sed 's/\.secret\.sops//')"
      - find {{.KUBERNETES_DIR}}/{{.cluster}}/bootstrap/talos/groups -type f | xargs -I{} curl -skT {} -u "{{.HOME_SERVICE_USER}}:" sftp://{{.HOME_SERVICE_ADDR}}/{{.HOME_SERVICE_MATCHBOX_DIR}}/groups/
      - find {{.KUBERNETES_DIR}}/{{.cluster}}/bootstrap/talos/profiles -type f | xargs -I{} curl -skT {} -u "{{.HOME_SERVICE_USER}}:" sftp://{{.HOME_SERVICE_ADDR}}/{{.HOME_SERVICE_MATCHBOX_DIR}}/profiles/
      - ssh -l {{.HOME_SERVICE_USER}} {{.HOME_SERVICE_ADDR}} "cd /var/opt/home-service ; go-task restart-matchbox"
    vars: *env-vars
    requires:
      vars: ["cluster"]
